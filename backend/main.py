# backend/main.py
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from fastapi.middleware.cors import CORSMiddleware
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# å¼•å…¥ä¹‹å‰çš„çˆ¬è™«å’ŒçŸ¥è¯†åº“
from crawler import get_real_time_trends 
# ğŸ‘‡ åŠ¡å¿…å¯¼å…¥ STYLE_SAMPLES
from knowledge_base import STYLE_KNOWLEDGE_BASE, TITLE_PROMPT, CONTENT_PROMPT, STYLE_SAMPLES
from knowledge_base import TRENDING_TOPICS as FALLBACK_TRENDS 
from utils import parse_file_content
import traceback

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

MODEL_MAP = {
    "DeepSeek V3 (ç§‘ç ”åˆ†æ)": "deepseek-v3",
    "Qwen Max (åˆ›æ„å†™ä½œ)": "qwen-max", 
    "Qwen Plus (é€»è¾‘æ¢³ç†)": "qwen-plus"
}

def get_llm(api_key, model_key, temperature):
    try:
        clean_key = api_key.strip()
        actual_model = MODEL_MAP.get(model_key, "deepseek-v3")
        print(f"ğŸ”§ [è°ƒè¯•] LLM Init: {actual_model}, Temp={temperature}")
        return ChatOpenAI(
            api_key=clean_key,
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1",
            model=actual_model,
            temperature=temperature
        )
    except Exception as e:
        print(f"âŒ LLM åˆå§‹åŒ–å¤±è´¥: {str(e)}")
        raise e

# --- æ¥å£ 1: è·å–é…ç½® ---
@app.get("/config")
async def get_config():
    print("ğŸŒ æ­£åœ¨è·å–çƒ­ç‚¹æ•°æ®...")
    real_trends = get_real_time_trends()
    
    if real_trends:
        print(f"âœ… çˆ¬è™«æˆåŠŸï¼Œè·å–åˆ° {len(real_trends)} æ¡çƒ­ç‚¹")
        final_trends = real_trends
    else:
        print("âš ï¸ çˆ¬è™«æœªè·å–åˆ°æ•°æ®ï¼Œä½¿ç”¨ Mock æ•°æ®å…œåº•")
        final_trends = FALLBACK_TRENDS

    return {
        "styles": list(STYLE_KNOWLEDGE_BASE.keys()),
        "trends": final_trends,
        "models": list(MODEL_MAP.keys())
    }

# --- æ¥å£ 2: ç”Ÿæˆæ ‡é¢˜ (æ”¹ç”¨ Form æ¥æ”¶æ–‡ä»¶) ---
@app.post("/generate_titles")
async def generate_titles(
    api_key: str = Form(...),
    subject: str = Form(...),
    style_key: str = Form(...),
    model_key: str = Form(...),
    creativity: float = Form(0.8),
    file: UploadFile = File(None) # æ”¯æŒæ–‡ä»¶ä¸Šä¼ 
):
    try:
        print(f"ğŸ“© [è¯·æ±‚] ç”Ÿæˆæ ‡é¢˜: ä¸»é¢˜={subject}")
        
        # 1. è§£ææ–‡ä»¶ (RAG)
        ref_summary = ""
        if file:
            content = await file.read()
            full_text = parse_file_content(content, file.filename)
            # æˆªå–å‰1000å­—ä½œä¸ºæ ‡é¢˜ç”Ÿæˆçš„å‚è€ƒæ‘˜è¦
            ref_summary = full_text[:1000]
            print(f"ğŸ“„ æ–‡ä»¶å·²è§£æ: {file.filename}, æå–æ‘˜è¦é•¿åº¦: {len(ref_summary)}")

        llm = get_llm(api_key, model_key, temperature=creativity)
        
        # 2. è·å–é£æ ¼å’Œæ ·æœ¬ (Few-Shot)
        style_guide = STYLE_KNOWLEDGE_BASE.get(style_key, "")
        # ğŸ‘‡ åŠ¨æ€è·å–å¯¹åº”çš„æ ·æœ¬ï¼Œæ²¡æ‰¾åˆ°å°±ç»™ç©ºå­—ç¬¦ä¸²
        examples = STYLE_SAMPLES.get(style_key, "æ— ç‰¹å®šå‚è€ƒèŒƒæ–‡")

        # 3. å¤„ç†çƒ­ç‚¹
        trends_text = "\n".join([f"- {t}" for t in FALLBACK_TRENDS])
        from crawler import CACHE_DATA
        if CACHE_DATA["trends"]:
             trends_text = "\n".join([f"- {t}" for t in CACHE_DATA["trends"]])

        prompt = ChatPromptTemplate.from_template(TITLE_PROMPT)
        chain = prompt | llm
        
        response = chain.invoke({
            "trends": trends_text, 
            "style": style_guide, 
            "subject": subject,
            "reference_summary": ref_summary,
            "examples": examples # ğŸ‘ˆ ä¼ å…¥æ ·æœ¬
        })
        return {"titles": response.content}

    except Exception as e:
        print("\nâŒ [ä¸¥é‡é”™è¯¯] ç”Ÿæˆæ ‡é¢˜å¤±è´¥:")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"åç«¯æŠ¥é”™: {str(e)}")

# --- æ¥å£ 3: ç”Ÿæˆå†…å®¹ (æ”¹ç”¨ Form æ¥æ”¶æ–‡ä»¶) ---
@app.post("/generate_content")
async def generate_content(
    api_key: str = Form(...),
    title: str = Form(...),
    content_type: str = Form(...),
    style_key: str = Form(...),
    model_key: str = Form(...),
    video_length: float = Form(1.0),
    creativity: float = Form(0.5),
    file: UploadFile = File(None) # æ”¯æŒæ–‡ä»¶ä¸Šä¼ 
):
    try:
        print(f"ğŸ“© [è¯·æ±‚] ç”Ÿæˆå†…å®¹: {title}")
        
        # 1. è§£ææ–‡ä»¶ (RAG)
        ref_material = "æ— å‚è€ƒèµ„æ–™ï¼Œè¯·åŸºäºé€šç”¨çŸ¥è¯†åˆ›ä½œã€‚"
        if file:
            content = await file.read()
            ref_material = parse_file_content(content, file.filename)
            print(f"ğŸ“„ RAGæ–‡ä»¶æ³¨å…¥æˆåŠŸï¼Œé•¿åº¦: {len(ref_material)}")
        
        llm = get_llm(api_key, model_key, temperature=creativity)
        
        # 2. è·å–é£æ ¼å’Œæ ·æœ¬
        style_guide = STYLE_KNOWLEDGE_BASE.get(style_key, "")
        examples = STYLE_SAMPLES.get(style_key, "æ— ç‰¹å®šå‚è€ƒèŒƒæ–‡") # ğŸ‘ˆ è·å–æ ·æœ¬
        
        prompt = ChatPromptTemplate.from_template(CONTENT_PROMPT)
        chain = prompt | llm
        
        response = chain.invoke({
            "type": content_type,
            "title": title,
            "style": style_guide,
            "duration": video_length,
            "reference_material": ref_material,
            "examples": examples # ğŸ‘ˆ ä¼ å…¥æ ·æœ¬
        })
        return {"content": response.content}

    except Exception as e:
        print("\nâŒ [ä¸¥é‡é”™è¯¯] ç”Ÿæˆå†…å®¹å¤±è´¥:")
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=f"åç«¯æŠ¥é”™: {str(e)}")